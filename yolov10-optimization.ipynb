{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9d010ff-85ba-4e69-b439-ba89e4a3a715",
   "metadata": {},
   "source": [
    "# Convert and Optimize YOLOv10 with OpenVINO\n",
    "\n",
    "Real-time object detection aims to accurately predict object categories and positions in images with low latency. The YOLO series has been at the forefront of this research due to its balance between performance and efficiency. However, reliance on NMS and architectural inefficiencies have hindered optimal performance. YOLOv10 addresses these issues by introducing consistent dual assignments for NMS-free training and a holistic efficiency-accuracy driven model design strategy.\n",
    "\n",
    "YOLOv10, built on the [Ultralytics Python package](https://pypi.org/project/ultralytics/) by researchers at [Tsinghua University](https://www.tsinghua.edu.cn/en/), introduces a new approach to real-time object detection, addressing both the post-processing and model architecture deficiencies found in previous YOLO versions. By eliminating non-maximum suppression (NMS) and optimizing various model components, YOLOv10 achieves state-of-the-art performance with significantly reduced computational overhead. Extensive experiments demonstrate its superior accuracy-latency trade-offs across multiple model scales.\n",
    "\n",
    "![yolov10-approach.png](https://github.com/ultralytics/ultralytics/assets/26833433/f9b1bec0-928e-41ce-a205-e12db3c4929a)\n",
    "\n",
    "More details about model architecture you can find in original [repo](https://github.com/THU-MIG/yolov10), [paper](https://arxiv.org/abs/2405.14458) and [Ultralytics documentation](https://docs.ultralytics.com/models/yolov10/).\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to run and optimize PyTorch YOLO V10 with OpenVINO.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "\n",
    "- Prepare PyTorch model\n",
    "- Convert PyTorch model to OpenVINO IR\n",
    "- Run model inference with OpenVINO\n",
    "- Prepare and run optimization pipeline using NNCF\n",
    "- Compare performance of the FP16 and quantized models.\n",
    "- Run optimized model inference on video\n",
    "- Launch interactive Gradio demo\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Download PyTorch model](#Download-PyTorch-model)\n",
    "- [Export PyTorch model to OpenVINO IR Format](#Export-PyTorch-model-to-OpenVINO-IR-Format)\n",
    "- [Run OpenVINO Inference on AUTO device using Ultralytics API](#Run-OpenVINO-Inference-on-AUTO-device-using-Ultralytics-API)\n",
    "- [Run OpenVINO Inference on selected device using Ultralytics API](#Run-OpenVINO-Inference-on-selected-device-using-Ultralytics-API)\n",
    "- [Optimize model using NNCF Post-training Quantization API](#Optimize-model-using-NNCF-Post-training-Quantization-API)\n",
    "    - [Prepare Quantization Dataset](#Prepare-Quantization-Dataset)\n",
    "    - [Quantize and Save INT8 model](#Quantize-and-Save-INT8-model)\n",
    "- [Run Optimized Model Inference](#Run-Optimized-Model-Inference)\n",
    "    - [Run Optimized Model on AUTO device](#Run-Optimized-Model-on-AUTO-device)\n",
    "    - [Run Optimized Model Inference on selected device](#Run-Optimized-Model-Inference-on-selected-device)\n",
    "- [Compare the Original and Quantized Models](#Compare-the-Original-and-Quantized-Models)\n",
    "    - [Model size](#Model-size)\n",
    "    - [Performance](#Performance)\n",
    "    - [FP16 model performance](#FP16-model-performance)\n",
    "    - [Int8 model performance](#Int8-model-performance)\n",
    "- [Live demo](#Live-demo)\n",
    "    - [Gradio Interactive Demo](#Gradio-Interactive-Demo)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/yolov10-optimization/yolov10-optimization.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144e86a1-c220-4e3b-a8b2-8eda443faf2d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76580b-860a-4dcd-8b01-f494cdffe37e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "%pip install -q \"nncf>=2.11.0\"\n",
    "%pip install -Uq \"openvino>=2024.3.0\"\n",
    "%pip install -q \"git+https://github.com/THU-MIG/yolov10.git\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"torch>=2.1\" \"torchvision>=0.16\" tqdm opencv-python \"gradio>=4.19\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75772eac-565b-4234-82bf-f305e0c1ceda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import download_file, VideoPlayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2888d8b4-44f6-4418-bae4-f433ff28010b",
   "metadata": {},
   "source": [
    "## Download PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "There are several version of [YOLO V10](https://github.com/THU-MIG/yolov10/tree/main?tab=readme-ov-file#performance) models provided by model authors. Each of them has different characteristics depends on number of training parameters, performance and accuracy. For demonstration purposes we will use `yolov10n`, but the same steps are also applicable to other models in YOLO V10 series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6315b-6a29-4f2a-96a9-eb3c1646a3a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828401e-b6bd-4796-a7b8-681957a159d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_weights_url = \"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt\"\n",
    "file_name = model_weights_url.split(\"/\")[-1]\n",
    "model_name = file_name.replace(\".pt\", \"\")\n",
    "\n",
    "download_file(model_weights_url, directory=models_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc21487d-dd79-4b56-b8e2-5b28c8d92ac8",
   "metadata": {},
   "source": [
    "## Export PyTorch model to OpenVINO IR Format\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cfec0d6-fb55-4611-861b-326e892fcf09",
   "metadata": {},
   "source": [
    "As it was discussed before, YOLO V10 code is designed on top of [Ultralytics](https://docs.ultralytics.com/) library and has similar interface with YOLO V8 (You can check [YOLO V8 notebooks](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/yolov8-optimization) for more detailed instruction how to work with Ultralytics API). Ultralytics support OpenVINO model export using [export](https://docs.ultralytics.com/modes/export/) method of model class. Additionally, we can specify parameters responsible for target input size, static or dynamic input shapes and model precision (FP32/FP16/INT8). INT8 quantization can be additionally performed on export stage, but for making approach more flexible, we consider how to perform quantization using [NNCF](https://github.com/openvinotoolkit/nncf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488f564-c1d5-4c6a-9b4a-353a3ab749e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import types\n",
    "from ultralytics.utils import ops, yaml_load, yaml_save\n",
    "from ultralytics import YOLOv10\n",
    "import torch\n",
    "\n",
    "detection_labels = {\n",
    "    0: \"person\",\n",
    "    1: \"bicycle\",\n",
    "    2: \"car\",\n",
    "    3: \"motorcycle\",\n",
    "    4: \"airplane\",\n",
    "    5: \"bus\",\n",
    "    6: \"train\",\n",
    "    7: \"truck\",\n",
    "    8: \"boat\",\n",
    "    9: \"traffic light\",\n",
    "    10: \"fire hydrant\",\n",
    "    11: \"stop sign\",\n",
    "    12: \"parking meter\",\n",
    "    13: \"bench\",\n",
    "    14: \"bird\",\n",
    "    15: \"cat\",\n",
    "    16: \"dog\",\n",
    "    17: \"horse\",\n",
    "    18: \"sheep\",\n",
    "    19: \"cow\",\n",
    "    20: \"elephant\",\n",
    "    21: \"bear\",\n",
    "    22: \"zebra\",\n",
    "    23: \"giraffe\",\n",
    "    24: \"backpack\",\n",
    "    25: \"umbrella\",\n",
    "    26: \"handbag\",\n",
    "    27: \"tie\",\n",
    "    28: \"suitcase\",\n",
    "    29: \"frisbee\",\n",
    "    30: \"skis\",\n",
    "    31: \"snowboard\",\n",
    "    32: \"sports ball\",\n",
    "    33: \"kite\",\n",
    "    34: \"baseball bat\",\n",
    "    35: \"baseball glove\",\n",
    "    36: \"skateboard\",\n",
    "    37: \"surfboard\",\n",
    "    38: \"tennis racket\",\n",
    "    39: \"bottle\",\n",
    "    40: \"wine glass\",\n",
    "    41: \"cup\",\n",
    "    42: \"fork\",\n",
    "    43: \"knife\",\n",
    "    44: \"spoon\",\n",
    "    45: \"bowl\",\n",
    "    46: \"banana\",\n",
    "    47: \"apple\",\n",
    "    48: \"sandwich\",\n",
    "    49: \"orange\",\n",
    "    50: \"broccoli\",\n",
    "    51: \"carrot\",\n",
    "    52: \"hot dog\",\n",
    "    53: \"pizza\",\n",
    "    54: \"donut\",\n",
    "    55: \"cake\",\n",
    "    56: \"chair\",\n",
    "    57: \"couch\",\n",
    "    58: \"potted plant\",\n",
    "    59: \"bed\",\n",
    "    60: \"dining table\",\n",
    "    61: \"toilet\",\n",
    "    62: \"tv\",\n",
    "    63: \"laptop\",\n",
    "    64: \"mouse\",\n",
    "    65: \"remote\",\n",
    "    66: \"keyboard\",\n",
    "    67: \"cell phone\",\n",
    "    68: \"microwave\",\n",
    "    69: \"oven\",\n",
    "    70: \"toaster\",\n",
    "    71: \"sink\",\n",
    "    72: \"refrigerator\",\n",
    "    73: \"book\",\n",
    "    74: \"clock\",\n",
    "    75: \"vase\",\n",
    "    76: \"scissors\",\n",
    "    77: \"teddy bear\",\n",
    "    78: \"hair drier\",\n",
    "    79: \"toothbrush\",\n",
    "}\n",
    "\n",
    "\n",
    "def v10_det_head_forward(self, x):\n",
    "    one2one = self.forward_feat([xi.detach() for xi in x], self.one2one_cv2, self.one2one_cv3)\n",
    "    if not self.export:\n",
    "        one2many = super().forward(x)\n",
    "\n",
    "    if not self.training:\n",
    "        one2one = self.inference(one2one)\n",
    "        if not self.export:\n",
    "            return {\"one2many\": one2many, \"one2one\": one2one}\n",
    "        else:\n",
    "            assert self.max_det != -1\n",
    "            boxes, scores, labels = ops.v10postprocess(one2one.permute(0, 2, 1), self.max_det, self.nc)\n",
    "            return torch.cat(\n",
    "                [boxes, scores.unsqueeze(-1), labels.unsqueeze(-1).to(boxes.dtype)],\n",
    "                dim=-1,\n",
    "            )\n",
    "    else:\n",
    "        return {\"one2many\": one2many, \"one2one\": one2one}\n",
    "\n",
    "half_precision = True\n",
    "if half_precision:\n",
    "    ov_model_path = models_dir / f\"FP16_openvino_model/{model_name}.xml\"\n",
    "else:\n",
    "    ov_model_path = models_dir / f\"FP32_openvino_model/{model_name}.xml\"\n",
    "\n",
    "# ov_model_path = models_dir / f\"{model_name}_openvino_model/{model_name}.xml\"\n",
    "# if not ov_model_path.exists():\n",
    "#     model = YOLOv10(models_dir / file_name)\n",
    "#     model.model.model[-1].forward = types.MethodType(v10_det_head_forward, model.model.model[-1])\n",
    "#     model.export(format=\"openvino\", dynamic=True, half=half_precision)\n",
    "#     config = yaml_load(ov_model_path.parent / \"metadata.yaml\")\n",
    "#     config[\"names\"] = detection_labels\n",
    "#     yaml_save(ov_model_path.parent / \"metadata.yaml\", config)\n",
    "if not ov_model_path.exists():\n",
    "    model = YOLOv10(models_dir / file_name)\n",
    "    model.model.model[-1].forward = types.MethodType(v10_det_head_forward, model.model.model[-1])\n",
    "    model.export(format=\"openvino\", dynamic=True, half=True)\n",
    "    # config = yaml_load(ov_model_path.parent / \"metadata.yaml\")\n",
    "    # config[\"names\"] = detection_labels\n",
    "    # yaml_save(ov_model_path.parent / \"metadata.yaml\", config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a23f621-f6cd-4bfd-8b98-24b8c0392761",
   "metadata": {},
   "source": [
    "## Run OpenVINO Inference on AUTO device using Ultralytics API\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "Now, when we exported model to OpenVINO, we can load it directly into YOLOv10 class, where automatic inference backend will provide easy-to-use user experience to run OpenVINO YOLOv10 model on the similar level like for original PyTorch model. The code bellow demonstrates how to run inference OpenVINO exported model with Ultralytics API on single image. [AUTO device](https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/auto-device) will be used for launching model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ed361-bf09-4398-ba15-e6c5dda73cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ov_yolo_model = YOLOv10(ov_model_path.parent, task=\"detect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a473286-59b8-498f-8541-df84f041e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = Path(\"./data/coco_bike.jpg\")\n",
    "download_file(\n",
    "    url=\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_bike.jpg\",\n",
    "    filename=IMAGE_PATH.name,\n",
    "    directory=IMAGE_PATH.parent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e116e81-0ad4-4b9e-9f5c-58680fd8f0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = ov_yolo_model(IMAGE_PATH, iou=0.45, conf=0.2)\n",
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "943b9b04-4b97-4395-99d1-695465de1140",
   "metadata": {},
   "source": [
    "## Run OpenVINO Inference on selected device using Ultralytics API\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "In this part of notebook you can select inference device for running model inference to compare results with AUTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41655bec-b006-4376-abbb-d6c8c09e89fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284d5fd-12ba-4c08-afce-d8131ef7ad4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ov_model = core.read_model(ov_model_path)\n",
    "\n",
    "# load model on selected device\n",
    "if \"GPU\" in device.value or \"NPU\" in device.value:\n",
    "    ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "ov_config = {}\n",
    "if \"GPU\" in device.value:\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "det_compiled_model = core.compile_model(ov_model, device.value, ov_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121068cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ov_yolo_model.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12164d21-948a-4ef7-9e1c-b0d41c53cf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ov_yolo_model.predictor.model.ov_compiled_model = det_compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9d396-ab1e-4398-a110-18c08b00ab14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = ov_yolo_model(IMAGE_PATH, iou=0.45, conf=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4abf758-3a72-4ee0-afde-38076602f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "992b6ac3-4248-4848-8fd6-f9c1b9fc2051",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Post-training Quantization API\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "We will use 8-bit quantization in post-training mode (without the fine-tuning pipeline) to optimize YOLOv10.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a Dataset for quantization.\n",
    "2. Run `nncf.quantize` for getting an optimized model.\n",
    "3. Serialize OpenVINO IR model, using the `openvino.save_model` function.\n",
    "\n",
    "Quantization is time and memory consuming process, you can skip this step using checkbox bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d9ce5-8df9-4803-ae85-bf0744de914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "int8_model_det_path = models_dir / \"INT8_openvino_model\" / f\"{model_name}.xml\"\n",
    "ov_yolo_int8_model = None\n",
    "\n",
    "to_quantize = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Quantization\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "to_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52287a23-b5ef-4c34-873a-32b398d26d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch skip_kernel_extension module\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
    ")\n",
    "open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
    "\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89ba64af-aa73-4ed9-a2f9-527bd9ae8221",
   "metadata": {},
   "source": [
    "### Prepare Quantization Dataset\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "For starting quantization, we need to prepare dataset. We will use validation subset from [MS COCO dataset](https://cocodataset.org/) for model quantization and Ultralytics validation data loader for preparing input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10435e-9cd6-4f29-a865-721d6209314d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from ultralytics.data.utils import DATASETS_DIR\n",
    "\n",
    "if not int8_model_det_path.exists():\n",
    "\n",
    "    DATA_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "    LABELS_URL = \"https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip\"\n",
    "    CFG_URL = \"https://raw.githubusercontent.com/ultralytics/ultralytics/v8.1.0/ultralytics/cfg/datasets/coco.yaml\"\n",
    "    \n",
    "    OUT_DIR = DATASETS_DIR\n",
    "    \n",
    "    DATA_PATH = OUT_DIR / \"val2017.zip\"\n",
    "    LABELS_PATH = OUT_DIR / \"coco2017labels-segments.zip\"\n",
    "    CFG_PATH = OUT_DIR / \"coco.yaml\"\n",
    "    \n",
    "    download_file(DATA_URL, DATA_PATH.name, DATA_PATH.parent)\n",
    "    download_file(LABELS_URL, LABELS_PATH.name, LABELS_PATH.parent)\n",
    "    download_file(CFG_URL, CFG_PATH.name, CFG_PATH.parent)\n",
    "    \n",
    "    if not (OUT_DIR / \"coco/labels\").exists():\n",
    "        with ZipFile(LABELS_PATH, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(OUT_DIR)\n",
    "        with ZipFile(DATA_PATH, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(OUT_DIR / \"coco/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a14818-21fd-4c71-8e58-2297dfafaadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "from ultralytics.cfg import get_cfg\n",
    "from ultralytics.data.converter import coco80_to_coco91_class\n",
    "from ultralytics.data.utils import check_det_dataset\n",
    "\n",
    "if not int8_model_det_path.exists():\n",
    "    args = get_cfg(cfg=DEFAULT_CFG)\n",
    "    args.data = str(CFG_PATH)\n",
    "    det_validator = ov_yolo_model.task_map[ov_yolo_model.task][\"validator\"](args=args)\n",
    "    \n",
    "    det_validator.data = check_det_dataset(args.data)\n",
    "    det_validator.stride = 32\n",
    "    det_data_loader = det_validator.get_dataloader(OUT_DIR / \"coco\", 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83fa49f4-e40c-48c7-82a6-8ef56e20b343",
   "metadata": {},
   "source": [
    "NNCF provides `nncf.Dataset` wrapper for using native framework dataloaders in quantization pipeline. Additionally, we specify transform function that will be responsible for preparing input data in model expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f0e0f-131e-43b9-b1be-639d4b3d1fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import nncf\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def transform_fn(data_item:Dict):\n",
    "    \"\"\"\n",
    "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
    "    Parameters:\n",
    "       data_item: Dict with data item produced by DataLoader during iteration\n",
    "    Returns:\n",
    "        input_tensor: Input data for quantization\n",
    "    \"\"\"\n",
    "    input_tensor = det_validator.preprocess(data_item)['img'].numpy()\n",
    "    return input_tensor\n",
    "\n",
    "if not int8_model_det_path.exists():\n",
    "    quantization_dataset = nncf.Dataset(det_data_loader, transform_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff8a6372-2097-4308-a4e5-e4651242a8df",
   "metadata": {},
   "source": [
    "### Quantize and Save INT8 model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "The `nncf.quantize` function provides an interface for model quantization. It requires an instance of the OpenVINO Model and quantization dataset. \n",
    "Optionally, some additional parameters for the configuration quantization process (number of samples for quantization, preset, ignored scope, etc.) can be provided. YOLOv10 model contains non-ReLU activation functions, which require asymmetric quantization of activations. To achieve a better result, we will use a `mixed` quantization preset. It provides symmetric quantization of weights and asymmetric quantization of activations.\n",
    "\n",
    ">**Note**: Model post-training quantization is time-consuming process. Be patient, it can take several minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12ae38-239e-4fe1-8296-47567f98538c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "import shutil\n",
    "\n",
    "if not int8_model_det_path.exists():\n",
    "    quantized_det_model = nncf.quantize(\n",
    "        ov_model,\n",
    "        quantization_dataset,\n",
    "        preset=nncf.QuantizationPreset.MIXED,\n",
    "    )\n",
    "\n",
    "    ov.save_model(quantized_det_model,  int8_model_det_path)\n",
    "    shutil.copy(ov_model_path.parent / \"metadata.yaml\", int8_model_det_path.parent / \"metadata.yaml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41b8a3eb-adb8-40cf-8602-311ff8d75a76",
   "metadata": {},
   "source": [
    "## Run Optimized Model Inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "The way of usage INT8 quantized model is the same like for model before quantization. Let's check inference result of quantized model on single image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f438df41-d614-4cca-a746-a173d92dddf5",
   "metadata": {},
   "source": [
    "### Run Optimized Model on AUTO device\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af61303b-16bd-4206-b926-2c28b3a859a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "ov_yolo_int8_model = YOLOv10(int8_model_det_path.parent, task=\"detect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13b8dd-49a1-4046-8a1a-491eee095a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "res = ov_yolo_int8_model(IMAGE_PATH, iou=0.45, conf=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423ba31-f0ee-4bfc-a18a-f7b870a9683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b4f7118-e046-4f87-b403-87053f2b0ac3",
   "metadata": {},
   "source": [
    "### Run Optimized Model Inference on selected device\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f22ae-6a8d-4578-b97f-4d9c77123418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43767a-1994-4202-a411-4738c92223da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%skip not $to_quantize.value\n",
    "\n",
    "ov_config = {}\n",
    "if \"GPU\" in device.value or \"NPU\" in device.value:\n",
    "    ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "ov_config = {}\n",
    "if \"GPU\" in device.value:\n",
    "    # ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "    ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"NO\"}\n",
    "\n",
    "quantized_det_model = core.read_model(int8_model_det_path)\n",
    "quantized_det_compiled_model = core.compile_model(quantized_det_model, device.value, ov_config)\n",
    "\n",
    "ov_yolo_int8_model.predictor.model.ov_compiled_model = quantized_det_compiled_model\n",
    "\n",
    "res = ov_yolo_int8_model(IMAGE_PATH,  iou=0.45, conf=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d64c7b-7595-41ca-87dd-4c85308f84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(res[0].plot()[:, :, ::-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7b70f75-a706-4f24-a487-88a3ad645dd5",
   "metadata": {},
   "source": [
    "## Compare the Original and Quantized Models\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6561246-a9e0-4cdf-8207-7e2f919d8582",
   "metadata": {},
   "source": [
    "### Model size\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d84d15-75be-4430-a58b-61cfd8e08dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model_weights = ov_model_path.with_suffix(\".bin\")\n",
    "if half_precision:\n",
    "    print(f\"Size of FP16 model is {ov_model_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(f\"Size of FP32 model is {ov_model_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "if int8_model_det_path.exists():\n",
    "    ov_int8_weights = int8_model_det_path.with_suffix(\".bin\")\n",
    "    print(f\"Size of model with INT8 compressed weights is {ov_int8_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"Compression rate for INT8 model: {ov_model_weights.stat().st_size / ov_int8_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f72b3d84-91f3-493c-8f4e-46598ffbd6d1",
   "metadata": {},
   "source": [
    "### Performance\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "### FP16 model performance\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ea508-e007-4313-ab0f-bfa0e3906176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!benchmark_app -m $ov_model_path -d $device.value -api async -shape \"[1,3,640,640]\" -t 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9852a8ef-4c45-43dd-a9ef-53ca6f13c99e",
   "metadata": {},
   "source": [
    "### Int8 model performance\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254626d-6c4f-4ca3-936f-a7c47e402e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if int8_model_det_path.exists():\n",
    "    !benchmark_app -m $int8_model_det_path -d $device.value -api async -shape \"[1,3,640,640]\" -t 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12d35bc1-8101-45ea-8bd7-53720b98f5e5",
   "metadata": {},
   "source": [
    "## Live demo\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "\n",
    "The following code runs model inference on a video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bd379-b1d5-4eb2-928f-97c7c2455a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from IPython import display\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Main processing function to run object detection.\n",
    "def run_object_detection(\n",
    "    source=0,\n",
    "    flip=False,\n",
    "    use_popup=False,\n",
    "    skip_first_frames=0,\n",
    "    det_model=ov_yolo_int8_model,\n",
    "    device=device.value,\n",
    "):\n",
    "    player = None\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = VideoPlayer(source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames)\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            # Get the results.\n",
    "            input_image = np.array(frame)\n",
    "\n",
    "            start_time = time.time()\n",
    "            detections = det_model(input_image, iou=0.45, conf=0.2, verbose=False)\n",
    "            stop_time = time.time()\n",
    "            frame = detections[0].plot()\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db930f4-fc41-4635-9248-deab2b6cfcb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_int8 = widgets.Checkbox(\n",
    "    value=ov_yolo_int8_model is not None,\n",
    "    description=\"Use int8 model\",\n",
    "    disabled=ov_yolo_int8_model is None,\n",
    ")\n",
    "\n",
    "use_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e37135-1bdc-40ff-8789-b5b4491cab84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WEBCAM_INFERENCE = False\n",
    "\n",
    "if WEBCAM_INFERENCE:\n",
    "    VIDEO_SOURCE = \"http://192.168.0.45:5000/video\"  # Webcam\n",
    "else:\n",
    "    # download_file(\n",
    "    #     \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/people.mp4\",\n",
    "    #     directory=\"data\",\n",
    "    # )\n",
    "    VIDEO_SOURCE = \"traffic.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9162a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(use_int8.value)\n",
    "print(device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6f47c-1c47-451d-89ba-0a29760f5ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(use_int8.value)\n",
    "run_object_detection(\n",
    "    det_model=ov_yolo_model if not use_int8.value else ov_yolo_int8_model,\n",
    "    source=VIDEO_SOURCE,\n",
    "    flip=False,\n",
    "    use_popup=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f612d80-68e2-45f8-aa62-9f3a05ca9de8",
   "metadata": {},
   "source": [
    "### Gradio Interactive Demo\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdbe248-7086-4804-b4b9-c3dd454bf80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def yolov10_inference(image, int8, conf_threshold, iou_threshold):\n",
    "    model = ov_yolo_model if not int8 else ov_yolo_int8_model\n",
    "    results = model(source=image, iou=iou_threshold, conf=conf_threshold, verbose=False)[0]\n",
    "    annotated_image = Image.fromarray(results.plot())\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\n",
    "        \"\"\"\n",
    "    <h1 style='text-align: center'>\n",
    "    YOLOv10: Real-Time End-to-End Object Detection using OpenVINO\n",
    "    </h1>\n",
    "    \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"numpy\", label=\"Image\")\n",
    "            conf_threshold = gr.Slider(\n",
    "                label=\"Confidence Threshold\",\n",
    "                minimum=0.1,\n",
    "                maximum=1.0,\n",
    "                step=0.1,\n",
    "                value=0.2,\n",
    "            )\n",
    "            iou_threshold = gr.Slider(\n",
    "                label=\"IoU Threshold\",\n",
    "                minimum=0.1,\n",
    "                maximum=1.0,\n",
    "                step=0.1,\n",
    "                value=0.45,\n",
    "            )\n",
    "            use_int8 = gr.Checkbox(\n",
    "                value=ov_yolo_int8_model is not None,\n",
    "                visible=ov_yolo_int8_model is not None,\n",
    "                label=\"Use INT8 model\",\n",
    "            )\n",
    "            yolov10_infer = gr.Button(value=\"Detect Objects\")\n",
    "\n",
    "        with gr.Column():\n",
    "            output_image = gr.Image(type=\"pil\", label=\"Annotated Image\")\n",
    "\n",
    "        yolov10_infer.click(\n",
    "            fn=yolov10_inference,\n",
    "            inputs=[\n",
    "                image,\n",
    "                use_int8,\n",
    "                conf_threshold,\n",
    "                iou_threshold,\n",
    "            ],\n",
    "            outputs=[output_image],\n",
    "        )\n",
    "    examples = gr.Examples(\n",
    "        [\n",
    "            \"data/coco_bike.jpg\",\n",
    "        ],\n",
    "        inputs=[\n",
    "            image,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/81ff3233-9c8d-4fe8-ab21-baf9ce530cff",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "YOLO"
    ],
    "tasks": [
     "Object Detection"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
